{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"font-family: 'Roboto', sans-serif;\"><center>游뱎 Reddit r/TikTokCringe EDA + Sentiment Analysis 游뱎</center></h1>\n\n***\n\n<br>\n<center><img src='https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExNzgzanIwZWx1cjd1Nmg0bms5MTFzd3g1ZmdqbW5lbDBtdnp4a3B4NiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9cw/VKVkmmopqMDieatVvG/giphy.gif' height=300px width=400px></center>\n<br>\n\n<div style=\"background-color:#20BEFF;\n            border-radius: 7px;\n            padding: 21px;\n            font-size:18px;\n            font-family: 'Roboto', sans-serif;\n            line-height: 1.7em;\">    \n<p style=\"color:white\">Please feel free to personalize or adapt the notebook according to your requirements. If you found the notebook useful, consider giving it an upvote. Your feedback and support are extremely valuable and serve as motivation for me to create more notebooks similar to this one.\n</div>","metadata":{"papermill":{"duration":0.012056,"end_time":"2023-06-14T22:58:09.0172","exception":false,"start_time":"2023-06-14T22:58:09.005144","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<br>","metadata":{"papermill":{"duration":0.008942,"end_time":"2023-06-14T22:58:09.035584","exception":false,"start_time":"2023-06-14T22:58:09.026642","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: 'Roboto', sans-serif;\"> 游 Introduction 游 </div></center>","metadata":{"papermill":{"duration":0.008181,"end_time":"2023-06-14T22:58:09.052465","exception":false,"start_time":"2023-06-14T22:58:09.044284","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"background-color:#20BEFF;\n            border-radius: 7px;\n            padding: 21px;\n            font-size:18px;\n            font-family: 'Roboto', sans-serif;\n            line-height: 1.7em;\">    \n<p style=\"color:white\">r/TikTokCringe is a subreddit dedicated to sharing and discussing cringe-worthy TikTok videos. Through data analysis and community engagement, we uncover trends, viral videos, and reactions that captivate the community.</p>\n<p style=\"color:white\">Analyzing the content helps us identify cringe-inducing trends, popular creators, and recurring themes. Engaging with the community provides an interactive space for laughter, discussions, and friendly banter.</p>\n<p style=\"color:white\">r/TikTokCringe embraces the lighthearted spirit of cringe, fostering a supportive environment where users unite in finding humor and enjoyment. It's a hub that appreciates the unique entertainment value that TikTok cringe provides.</p>\n<p style=\"color:white\">In conclusion, r/TikTokCringe is a lively subreddit where users share and explore cringe-worthy TikTok videos, creating a space for laughter and camaraderie through data analysis and community engagement.</p></div>","metadata":{"papermill":{"duration":0.008343,"end_time":"2023-06-14T22:58:09.069309","exception":false,"start_time":"2023-06-14T22:58:09.060966","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<br>","metadata":{"papermill":{"duration":0.008429,"end_time":"2023-06-14T22:58:09.086295","exception":false,"start_time":"2023-06-14T22:58:09.077866","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: 'Roboto', sans-serif;\"> 游닍 Packages 游닍 </div></center>\n<div style=\"background-color:#f2f2f2; padding: 20px; border-radius: 7px;\">\n<p style=\"font-size:17px; font-family:'Roboto', sans-serif; line-height: 1.7em;\">This code snippet imports libraries for performing data manipulation, using regular expressions, displaying progress bars, generating random values, performing natural language processing tasks like sentiment analysis, creating word clouds, and plotting data using various visualization libraries.","metadata":{"papermill":{"duration":0.008322,"end_time":"2023-06-14T22:58:09.10337","exception":false,"start_time":"2023-06-14T22:58:09.095048","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd  # Data manipulation\nimport pytz  # Time zones\nimport re  # Regular expressions\nfrom tqdm import tqdm  # Progress bars\nimport random  # Random values\n\nimport nltk  # Natural language processing\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA  # Sentiment analysis\nnltk.download('vader_lexicon')  # Download VADER lexicon\nfrom nltk.corpus import stopwords  # Stopwords\nnltk.download('stopwords')  # Download stopwords corpus\n\nfrom wordcloud import WordCloud  # Word cloud\n\nimport matplotlib.pyplot as plt  # Plotting\nimport seaborn as sns  # Statistical data visualization\nimport plotly.express as px  # Interactive plotting\nimport plotly.subplots as sp \n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":3.450753,"end_time":"2023-06-14T22:58:12.562689","exception":false,"start_time":"2023-06-14T22:58:09.111936","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:24.543473Z","iopub.execute_input":"2023-06-16T00:36:24.544179Z","iopub.status.idle":"2023-06-16T00:36:28.536138Z","shell.execute_reply.started":"2023-06-16T00:36:24.544138Z","shell.execute_reply":"2023-06-16T00:36:28.53441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{"papermill":{"duration":0.008519,"end_time":"2023-06-14T22:58:12.580416","exception":false,"start_time":"2023-06-14T22:58:12.571897","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: 'Roboto', sans-serif;\"> 游깷 Preparation 游깷 </div></center>\n<div style=\"background-color:#f2f2f2; padding: 20px; border-radius: 7px;\">\n<p style=\"font-size:17px; font-family:'Roboto', sans-serif; line-height: 1.7em;\">This code segment performs tasks such as setting the plotting style, initializing a sentiment analyzer, retrieving stopwords, and defining a custom color map.","metadata":{"papermill":{"duration":0.008464,"end_time":"2023-06-14T22:58:12.597618","exception":false,"start_time":"2023-06-14T22:58:12.589154","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Set the plotting style to 'darkgrid'.\nsns.set_style('darkgrid')\n\n# Create an instance of the SentimentIntensityAnalyzer for sentiment analysis.\nsia = SIA()\n\n# Get the stopwords for the Portuguese language.\nstpwrds = stopwords.words('english')\n\n# Define a custom color map for sentiment categories based on Kaggle palette.\ncustom_colors_map = {'Negative':'#FFA07A','Neutral':'#FFFFFF', 'Positive':'#20BEFF'}","metadata":{"papermill":{"duration":0.031564,"end_time":"2023-06-14T22:58:12.63812","exception":false,"start_time":"2023-06-14T22:58:12.606556","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:28.538437Z","iopub.execute_input":"2023-06-16T00:36:28.538823Z","iopub.status.idle":"2023-06-16T00:36:28.561307Z","shell.execute_reply.started":"2023-06-16T00:36:28.538798Z","shell.execute_reply":"2023-06-16T00:36:28.558906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{"papermill":{"duration":0.008606,"end_time":"2023-06-14T22:58:12.656043","exception":false,"start_time":"2023-06-14T22:58:12.647437","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: 'Roboto', sans-serif;\"> 游닌 Load 游닌 </div></center>\n<div style=\"background-color:#f2f2f2; padding: 20px; border-radius: 7px;\">\n<p style=\"font-size:17px; font-family:'Roboto', sans-serif; line-height: 1.7em;\">This section retrieves the latest posts from the subreddit. It collects information such as the post ID, author, datetime, title, URL, score, number of comments, post text, original post text, author's comment karma, and a tag associated with the post. The data is stored in a DataFrame named 'data', which allows for further analysis and exploration of the collected Reddit post information and it's comments.","metadata":{"papermill":{"duration":0.009,"end_time":"2023-06-14T22:58:12.674996","exception":false,"start_time":"2023-06-14T22:58:12.665996","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/rtiktokcringe-post-and-comments/data.csv').iloc[:,:12]","metadata":{"papermill":{"duration":0.223351,"end_time":"2023-06-14T22:58:12.907524","exception":false,"start_time":"2023-06-14T22:58:12.684173","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:28.563018Z","iopub.execute_input":"2023-06-16T00:36:28.56377Z","iopub.status.idle":"2023-06-16T00:36:28.814738Z","shell.execute_reply.started":"2023-06-16T00:36:28.563727Z","shell.execute_reply":"2023-06-16T00:36:28.813039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df.copy()","metadata":{"papermill":{"duration":0.018501,"end_time":"2023-06-14T22:58:12.935051","exception":false,"start_time":"2023-06-14T22:58:12.91655","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:28.818275Z","iopub.execute_input":"2023-06-16T00:36:28.818727Z","iopub.status.idle":"2023-06-16T00:36:28.826239Z","shell.execute_reply.started":"2023-06-16T00:36:28.818689Z","shell.execute_reply":"2023-06-16T00:36:28.825044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the collected data.\ndata.head()","metadata":{"papermill":{"duration":0.04873,"end_time":"2023-06-14T22:58:12.993156","exception":false,"start_time":"2023-06-14T22:58:12.944426","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:28.82778Z","iopub.execute_input":"2023-06-16T00:36:28.828167Z","iopub.status.idle":"2023-06-16T00:36:28.881615Z","shell.execute_reply.started":"2023-06-16T00:36:28.828131Z","shell.execute_reply":"2023-06-16T00:36:28.880387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{"papermill":{"duration":0.009219,"end_time":"2023-06-14T22:58:13.012809","exception":false,"start_time":"2023-06-14T22:58:13.00359","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: 'Roboto', sans-serif;\"> 游대 Transformation 游대 </div></center>\n<div style=\"background-color:#f2f2f2; padding: 20px; border-radius: 7px;\">\n<p style=\"font-size:17px; font-family:'Roboto', sans-serif; line-height: 1.7em;\">Here the code performs several operations on the data to prepare it for further analysis. It fills missing tags, counts the occurrences of each tag, and applies text transformation to the main text column. The text transformation includes lowercase conversion, removal of usernames, hashtags, and URLs, extraction of words, removal of stopwords, and the addition of new columns for word count, text length, and hour. These transformations help in cleaning and organizing the data, making it ready for subsequent analysis and exploration.","metadata":{"papermill":{"duration":0.008877,"end_time":"2023-06-14T22:58:13.031164","exception":false,"start_time":"2023-06-14T22:58:13.022287","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Fill tags with 'Sem Tag' if they are missing.\ndata.tag.fillna('Without Tag', inplace=True)\n\n# Count the occurrences of each tag in the 'tag' column of the 'data' DataFrame.\ntags = data.tag.value_counts()\n\n# Reset the index of the 'tags' Series to convert it into a DataFrame.\ntags = tags.reset_index()","metadata":{"papermill":{"duration":0.031286,"end_time":"2023-06-14T22:58:13.07161","exception":false,"start_time":"2023-06-14T22:58:13.040324","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:28.88312Z","iopub.execute_input":"2023-06-16T00:36:28.883482Z","iopub.status.idle":"2023-06-16T00:36:28.903076Z","shell.execute_reply.started":"2023-06-16T00:36:28.883453Z","shell.execute_reply":"2023-06-16T00:36:28.901562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General transformation in the text\ndef transformation(df, mc, ac):\n    # mc = main_column\n    # ac = another_column\n\n    # Replace empty main_column values with another_column values.\n    df[mc] = df.apply(lambda x: x[ac] if x[mc] == '' else x[mc], axis=1)\n    \n    df[mc] = df[mc].str.lower()\n    df[mc] = df[mc].apply(lambda x: re.sub('@[^\\s]+', '', x))\n    df[mc] = df[mc].apply(lambda x: re.sub(r'\\B#\\S+', '', x))\n    df[mc] = df[mc].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n    df[mc] = df[mc].apply(lambda x: ' '.join(re.findall(r'\\w+', x)))\n    df[mc] = df[mc].apply(lambda x: re.sub(r'\\s+[b-zA-Z]\\s+', ' ', x))\n    df[mc] = df[mc].apply(lambda x: re.sub(r'\\s+', ' ', x, flags=re.I))\n    df[mc] = df[mc].apply(lambda x: ' '.join([word for word in x.split() if word not in stpwrds]))\n\n    df['words'] = df[mc].apply(lambda x: re.findall(r'\\w+', x))\n    df['words_count'] = df.words.apply(len)\n    df['length'] = df[mc].apply(len)\n    df['hour'] = df.datetime.apply(lambda x: x.hour)\n\n    return df\n\ndata.datetime = pd.to_datetime(data.datetime)\ndata.text = data.text.astype(str)\ndata.title = data.title.astype(str)\n\ndata = transformation(data, 'text', 'title')","metadata":{"papermill":{"duration":2.366516,"end_time":"2023-06-14T22:58:15.447544","exception":false,"start_time":"2023-06-14T22:58:13.081028","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:28.904771Z","iopub.execute_input":"2023-06-16T00:36:28.905274Z","iopub.status.idle":"2023-06-16T00:36:31.234019Z","shell.execute_reply.started":"2023-06-16T00:36:28.905249Z","shell.execute_reply":"2023-06-16T00:36:31.232275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the transformed data.\ndata.head()","metadata":{"papermill":{"duration":0.038347,"end_time":"2023-06-14T22:58:15.495417","exception":false,"start_time":"2023-06-14T22:58:15.45707","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:31.235406Z","iopub.execute_input":"2023-06-16T00:36:31.235692Z","iopub.status.idle":"2023-06-16T00:36:31.260918Z","shell.execute_reply.started":"2023-06-16T00:36:31.23567Z","shell.execute_reply":"2023-06-16T00:36:31.259627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{"papermill":{"duration":0.009199,"end_time":"2023-06-14T22:58:15.514612","exception":false,"start_time":"2023-06-14T22:58:15.505413","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: 'Roboto', sans-serif;\"> 游녻 Sentiment Analysis 游녻 </div></center>\n<div style=\"background-color:#f2f2f2; padding: 20px; border-radius: 7px;\">\n<p style=\"font-size:17px; font-family:'Roboto', sans-serif; line-height: 1.7em;\">The code performs sentiment analysis on the text data using the VADER model from the NLTK library. It calculates sentiment scores and classifies them into three categories: 'Negative', 'Positive', or 'Neutral'. The sentiment scores are stored in the 'sentiment_eval' column, and the corresponding classifications are stored in the 'class_sentiment' column.","metadata":{"papermill":{"duration":0.009098,"end_time":"2023-06-14T22:58:15.533246","exception":false,"start_time":"2023-06-14T22:58:15.524148","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def sentiment_classification(x: float):\n    return 'Negative' if x < -0.25 else 'Positive' if x > 0.25 else 'Neutral'\n\ndata['sentiment_eval'] = [sia.polarity_scores(x)['compound'] for x in tqdm(data['text'])]\n\ndata['class_sentiment'] = data['sentiment_eval'].apply(sentiment_classification)","metadata":{"papermill":{"duration":5.425512,"end_time":"2023-06-14T22:58:20.968065","exception":false,"start_time":"2023-06-14T22:58:15.542553","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:31.263063Z","iopub.execute_input":"2023-06-16T00:36:31.263495Z","iopub.status.idle":"2023-06-16T00:36:36.28353Z","shell.execute_reply.started":"2023-06-16T00:36:31.26346Z","shell.execute_reply":"2023-06-16T00:36:36.282567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{"papermill":{"duration":0.01334,"end_time":"2023-06-14T22:58:20.994332","exception":false,"start_time":"2023-06-14T22:58:20.980992","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: 'Roboto', sans-serif;\"> 游늵 Exploratory Data Analysis and Visualization 游늵 </div></center>\n<div style=\"background-color:#f2f2f2; padding: 20px; border-radius: 7px;\">\n<p style=\"font-size:17px; font-family:'Roboto', sans-serif; line-height: 1.7em;\">In this section, we explore and visualize data using Plotly Express and WordCloud libraries. Through histograms, stacked bar plots, and word clouds, we reveal insights from sentiment analysis results based on tags and hours. Our objective is to compare the sentiment expressed in post data with comment data, uncovering patterns and similarities. Get ready for visually engaging visuals and captivating word clouds as we delve into the sentiment landscape of the dataset.","metadata":{"papermill":{"duration":0.01332,"end_time":"2023-06-14T22:58:21.021442","exception":false,"start_time":"2023-06-14T22:58:21.008122","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(f'{data.shape[0]} Rows')","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.023444,"end_time":"2023-06-14T22:58:21.058467","exception":false,"start_time":"2023-06-14T22:58:21.035023","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:36.286911Z","iopub.execute_input":"2023-06-16T00:36:36.287238Z","iopub.status.idle":"2023-06-16T00:36:36.29202Z","shell.execute_reply.started":"2023-06-16T00:36:36.287214Z","shell.execute_reply":"2023-06-16T00:36:36.291134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{data.shape[1]} Columns')","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.023548,"end_time":"2023-06-14T22:58:21.095615","exception":false,"start_time":"2023-06-14T22:58:21.072067","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:36.292957Z","iopub.execute_input":"2023-06-16T00:36:36.293242Z","iopub.status.idle":"2023-06-16T00:36:36.316819Z","shell.execute_reply.started":"2023-06-16T00:36:36.29322Z","shell.execute_reply":"2023-06-16T00:36:36.315792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract post rows.\ndata_post = data[data['comment_id'].isnull()]\n\n# Extract comment rows.\ndata_comment = data[data['comment_id'].notnull()]","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.038549,"end_time":"2023-06-14T22:58:21.14735","exception":false,"start_time":"2023-06-14T22:58:21.108801","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:36.31765Z","iopub.execute_input":"2023-06-16T00:36:36.317917Z","iopub.status.idle":"2023-06-16T00:36:36.355865Z","shell.execute_reply.started":"2023-06-16T00:36:36.317895Z","shell.execute_reply":"2023-06-16T00:36:36.353702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{"papermill":{"duration":0.013047,"end_time":"2023-06-14T22:58:21.173855","exception":false,"start_time":"2023-06-14T22:58:21.160808","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <center><div style=\"font-family: 'Roboto', sans-serif;\"> Count by Sentiment </div></center>","metadata":{"papermill":{"duration":0.012729,"end_time":"2023-06-14T22:58:21.199905","exception":false,"start_time":"2023-06-14T22:58:21.187176","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Function to create a custom histogram.\ndef senti_plot(df, type: str):\n    # type = 'post' or 'comment'\n    fig = px.histogram(\n    df['class_sentiment'],\n    y=\"class_sentiment\",\n    title=f'Sentiment Analysis from {type}',\n    color='class_sentiment',\n    color_discrete_map=custom_colors_map\n    )\n\n    # Customize the hover template to display the sentiment and count.\n    fig.update_traces(hovertemplate='%{y}<br>Count: %{x}')\n\n    # Update the layout with additional configurations.\n    fig.update_layout(template='plotly_dark', title_x=0.5, yaxis_title='Sentiment', xaxis_title='Count', legend_title='Sentiment')\n\n    # Display the figure.\n    return fig","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.023119,"end_time":"2023-06-14T22:58:21.236152","exception":false,"start_time":"2023-06-14T22:58:21.213033","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:36.357772Z","iopub.execute_input":"2023-06-16T00:36:36.358357Z","iopub.status.idle":"2023-06-16T00:36:36.365966Z","shell.execute_reply.started":"2023-06-16T00:36:36.358311Z","shell.execute_reply":"2023-06-16T00:36:36.364966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram from posts.\nsenti_plot(data_post, 'Posts')","metadata":{"_kg_hide-input":true,"papermill":{"duration":1.918518,"end_time":"2023-06-14T22:58:23.167558","exception":false,"start_time":"2023-06-14T22:58:21.24904","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:36.367431Z","iopub.execute_input":"2023-06-16T00:36:36.367777Z","iopub.status.idle":"2023-06-16T00:36:38.884408Z","shell.execute_reply.started":"2023-06-16T00:36:36.367752Z","shell.execute_reply":"2023-06-16T00:36:38.882584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram from comments.\nsenti_plot(data_comment, 'Comments')","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.159207,"end_time":"2023-06-14T22:58:23.340384","exception":false,"start_time":"2023-06-14T22:58:23.181177","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:38.885626Z","iopub.execute_input":"2023-06-16T00:36:38.885974Z","iopub.status.idle":"2023-06-16T00:36:39.067534Z","shell.execute_reply.started":"2023-06-16T00:36:38.885945Z","shell.execute_reply":"2023-06-16T00:36:39.066471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{"papermill":{"duration":0.014102,"end_time":"2023-06-14T22:58:23.369076","exception":false,"start_time":"2023-06-14T22:58:23.354974","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <center><div style=\"font-family: 'Roboto', sans-serif;\"> Percentage of Sentiment by Tag </div></center>","metadata":{"papermill":{"duration":0.014429,"end_time":"2023-06-14T22:58:23.397645","exception":false,"start_time":"2023-06-14T22:58:23.383216","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def tags_plot(df, type: str):\n    sentiment_by_tag = df.groupby(['tag', 'class_sentiment']).size().reset_index(name='counts')\n\n    # Pivot the data to reshape it with 'tag' as the index, 'class_sentiment' as columns, and 'counts' as values.\n    sentiment_by_tag = sentiment_by_tag.pivot(index='tag',columns='class_sentiment',values='counts').reset_index().fillna(0)\n\n    # Calculate the total count for each tag.\n    try:\n        sentiment_by_tag['Total'] = sentiment_by_tag['Negative'] + sentiment_by_tag['Neutral'] + sentiment_by_tag['Positive']\n    except:\n        sentiment_by_tag['Neutral'] = 0\n        sentiment_by_tag['Total'] = sentiment_by_tag['Negative'] + sentiment_by_tag['Neutral'] + sentiment_by_tag['Positive']\n\n    # Calculate the percentage of each sentiment category for each tag.\n    sentiment_by_tag['Negative'] = (sentiment_by_tag['Negative'] / sentiment_by_tag['Total']) * 100\n    sentiment_by_tag['Neutral'] = (sentiment_by_tag['Neutral'] / sentiment_by_tag['Total']) * 100\n    sentiment_by_tag['Positive'] = (sentiment_by_tag['Positive'] / sentiment_by_tag['Total']) * 100\n\n    # Create a stacked bar plot using Plotly Express.\n    fig = px.bar(\n        data_frame=sentiment_by_tag,\n        x='tag',\n        y=['Negative', 'Neutral', 'Positive'],\n        title=f'Sentiment Analysis by Tag from {type}',\n        barmode='stack',\n        color_discrete_map=custom_colors_map\n    )\n\n    # Customize the hover template to display the tag and percentage.\n    fig.update_traces(hovertemplate='%{x}<br>Percentage: %{y:.2f}')\n\n    # Update the layout with additional configurations.\n    fig.update_layout(template='plotly_dark', title_x=0.5, yaxis_title='Percentage', xaxis_title='Tag', legend_title='Sentiment', legend=dict(traceorder='reversed'))\n\n    # Update the trace names to title case.\n    fig.for_each_trace(lambda t: t.update(name=t.name.title()))\n\n    # Display the plot.\n    fig.show()","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.02827,"end_time":"2023-06-14T22:58:23.440009","exception":false,"start_time":"2023-06-14T22:58:23.411739","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:39.068796Z","iopub.execute_input":"2023-06-16T00:36:39.06918Z","iopub.status.idle":"2023-06-16T00:36:39.080921Z","shell.execute_reply.started":"2023-06-16T00:36:39.069151Z","shell.execute_reply":"2023-06-16T00:36:39.079456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hourly from posts.\ntags_plot(data_post, 'Posts')","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.155822,"end_time":"2023-06-14T22:58:23.61063","exception":false,"start_time":"2023-06-14T22:58:23.454808","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:39.082095Z","iopub.execute_input":"2023-06-16T00:36:39.082413Z","iopub.status.idle":"2023-06-16T00:36:39.234329Z","shell.execute_reply.started":"2023-06-16T00:36:39.082386Z","shell.execute_reply":"2023-06-16T00:36:39.232958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hourly from comments.\ntags_plot(data_comment, 'Comments')","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.113406,"end_time":"2023-06-14T22:58:23.738534","exception":false,"start_time":"2023-06-14T22:58:23.625128","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:39.235684Z","iopub.execute_input":"2023-06-16T00:36:39.236101Z","iopub.status.idle":"2023-06-16T00:36:39.33775Z","shell.execute_reply.started":"2023-06-16T00:36:39.236065Z","shell.execute_reply":"2023-06-16T00:36:39.336233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{"papermill":{"duration":0.014762,"end_time":"2023-06-14T22:58:23.768245","exception":false,"start_time":"2023-06-14T22:58:23.753483","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <center><div style=\"font-family: 'Roboto', sans-serif;\"> Count by Hour and Sentiment </div></center>","metadata":{"papermill":{"duration":0.014731,"end_time":"2023-06-14T22:58:23.798214","exception":false,"start_time":"2023-06-14T22:58:23.783483","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def format_hour(h: int):\n    h = str(h)\n    if len(h) == 1:\n        h = '0' + h\n    h = h + \":00\"\n    return h\n\ndef hourly_plot(df, type: str):\n    # Group the data by 'hour' and 'class_sentiment' columns and calculate the counts.\n    combined_hourly = df.groupby(['hour', 'class_sentiment']).size().reset_index(name='counts')\n    \n    # Correct the lack of data\n    if sum(combined_hourly['class_sentiment'] == 'Neutral') <= 0:\n        insert = pd.DataFrame.from_dict({'hour':[0], 'class_sentiment':['Neutral'], 'counts':[0]})\n        combined_hourly = combined_hourly.append(insert)\n\n    # Apply the 'format_hour' function to format the 'hour' column.\n    combined_hourly['hour'] = combined_hourly['hour'].apply(format_hour)\n\n    # Create a base DataFrame with all hours from 0 to 23.\n    hour_base = pd.DataFrame({'hour': [format_hour(x) for x in range(0, 24)]})\n\n    # Merge the base DataFrame with the grouped data to fill missing hours with 0 counts.\n    combined_hourly = pd.merge(hour_base, combined_hourly, on='hour', how='left')\n\n    # Pivot the data to reshape it with 'hour' as the index, sentiment categories as columns, and counts as values.\n    combined_hourly = combined_hourly.pivot(index='hour',columns='class_sentiment',values='counts').reset_index().fillna(0)\n\n    # Reorder the columns of the DataFrame.\n    combined_hourly = combined_hourly[['hour', 'Negative', 'Neutral', 'Positive']]\n\n    # Create a stacked bar plot using Plotly Express.\n    fig = px.bar(\n        data_frame=combined_hourly,\n        x='hour',\n        y=['Negative','Neutral','Positive'],\n        title=f'Sentiment Analysis by Hour from {type}',\n        barmode='stack',\n        color_discrete_map=custom_colors_map\n    )\n\n    # Customize the hover template to display the count.\n    fig.update_traces(hovertemplate='Count: %{y}')\n\n    # Update the layout with additional configurations.\n    fig.update_layout(template='plotly_dark', title_x=0.5, yaxis_title='Posts', xaxis_title='Hour', legend_title='Sentiment')\n\n    # Update the trace names to title case.\n    fig.for_each_trace(lambda t: t.update(name=t.name.title()))\n\n    # Set the legend order as Positive, Neutral, Negative.\n    fig.update_layout(legend=dict(traceorder='reversed'))\n\n    # Display the plot.\n    fig.show()","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.029305,"end_time":"2023-06-14T22:58:23.84243","exception":false,"start_time":"2023-06-14T22:58:23.813125","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:39.339239Z","iopub.execute_input":"2023-06-16T00:36:39.339604Z","iopub.status.idle":"2023-06-16T00:36:39.350945Z","shell.execute_reply.started":"2023-06-16T00:36:39.339574Z","shell.execute_reply":"2023-06-16T00:36:39.350232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hourly from posts.\nhourly_plot(data_post, 'Posts')","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.13236,"end_time":"2023-06-14T22:58:23.99054","exception":false,"start_time":"2023-06-14T22:58:23.85818","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:39.352129Z","iopub.execute_input":"2023-06-16T00:36:39.352508Z","iopub.status.idle":"2023-06-16T00:36:39.48641Z","shell.execute_reply.started":"2023-06-16T00:36:39.352476Z","shell.execute_reply":"2023-06-16T00:36:39.484782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hourly from comments.\nhourly_plot(data_comment, 'Comments')","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.119287,"end_time":"2023-06-14T22:58:24.126955","exception":false,"start_time":"2023-06-14T22:58:24.007668","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:39.487837Z","iopub.execute_input":"2023-06-16T00:36:39.488227Z","iopub.status.idle":"2023-06-16T00:36:39.602203Z","shell.execute_reply.started":"2023-06-16T00:36:39.488196Z","shell.execute_reply":"2023-06-16T00:36:39.600859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{"papermill":{"duration":0.015845,"end_time":"2023-06-14T22:58:24.159244","exception":false,"start_time":"2023-06-14T22:58:24.143399","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <center><div style=\"font-family: 'Roboto', sans-serif;\"> Word Cloud by Posts </div></center>","metadata":{"papermill":{"duration":0.01704,"end_time":"2023-06-14T22:58:24.193806","exception":false,"start_time":"2023-06-14T22:58:24.176766","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Function to flatten a nested list.\ndef flatten_list(l):\n    return [x for y in l for x in y]\n\n# Function to generate a colored WordCloud.\ndef generate_word_cloud(text, color_func, title, ax):\n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"black\", color_func=color_func).generate(text)\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.set_title(title, fontsize=30)\n    ax.axis(\"off\")\n\n# Function to generate word clouds.\ndef generate_word_clouds(pos_doc, neu_doc, neg_doc):\n    fig, axes = plt.subplots(1, 3, figsize=(20, 10))  # Change to 1 row and 3 columns\n\n    titles = [\"Positive Words\", \"Neutral Words\", \"Negative Words\"]\n    docs = [pos_doc, neu_doc, neg_doc]\n    color_funcs = [lambda *args, **kwargs: custom_colors_map['Positive'],\n                   lambda *args, **kwargs: custom_colors_map['Neutral'],\n                   lambda *args, **kwargs: custom_colors_map['Negative']]\n\n    for i, (doc, color_func, title) in enumerate(zip(docs, color_funcs, titles)):\n        generate_word_cloud(\" \".join(doc), color_func, title, axes[i])\n\n    plt.tight_layout()\n    plt.show()\n\ndef word_clouds(df):\n    # Sort data by 'hour' in descending order.\n    sentiment_sorted = df.sort_values('hour', ascending=False)\n\n    # Get top 100 records for each sentiment category\n    positive_top_100 = sentiment_sorted[sentiment_sorted['class_sentiment'] == \"Positive\"].iloc[:100]\n    negative_top_100 = sentiment_sorted[sentiment_sorted['class_sentiment'] == \"Negative\"].iloc[:100]\n    neutral_top_100 = sentiment_sorted[sentiment_sorted['class_sentiment'] == \"Neutral\"].iloc[:100]\n    \n    # Function to remove stopwords.\n    cleanup = lambda x: [y for y in x.split() if y not in stpwrds]\n    neg_doc = flatten_list(negative_top_100['text'].apply(cleanup))\n    pos_doc = flatten_list(positive_top_100['text'].apply(cleanup))\n        \n    if len(neutral_top_100) == 0: \n        neu_doc = ['_']\n    else:\n        neu_doc = flatten_list(neutral_top_100['text'].apply(cleanup))\n        \n    generate_word_clouds(pos_doc, neu_doc, neg_doc)","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.033469,"end_time":"2023-06-14T22:58:24.244242","exception":false,"start_time":"2023-06-14T22:58:24.210773","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:39.603539Z","iopub.execute_input":"2023-06-16T00:36:39.603891Z","iopub.status.idle":"2023-06-16T00:36:39.616726Z","shell.execute_reply.started":"2023-06-16T00:36:39.603861Z","shell.execute_reply":"2023-06-16T00:36:39.615581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word clouds from posts.\nword_clouds(data_post)","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.899338,"end_time":"2023-06-14T22:58:25.160036","exception":false,"start_time":"2023-06-14T22:58:24.260698","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:39.617823Z","iopub.execute_input":"2023-06-16T00:36:39.618226Z","iopub.status.idle":"2023-06-16T00:36:40.881059Z","shell.execute_reply.started":"2023-06-16T00:36:39.618193Z","shell.execute_reply":"2023-06-16T00:36:40.879466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{"papermill":{"duration":0.020589,"end_time":"2023-06-14T22:58:25.202064","exception":false,"start_time":"2023-06-14T22:58:25.181475","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <center><div style=\"font-family: 'Roboto', sans-serif;\"> Word Cloud by Comments </div></center>","metadata":{"papermill":{"duration":0.02099,"end_time":"2023-06-14T22:58:25.244073","exception":false,"start_time":"2023-06-14T22:58:25.223083","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Word clouds from comments.\nword_clouds(data_comment)","metadata":{"_kg_hide-input":true,"papermill":{"duration":1.331447,"end_time":"2023-06-14T22:58:26.596642","exception":false,"start_time":"2023-06-14T22:58:25.265195","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-16T00:36:40.883023Z","iopub.execute_input":"2023-06-16T00:36:40.883398Z","iopub.status.idle":"2023-06-16T00:36:42.300472Z","shell.execute_reply.started":"2023-06-16T00:36:40.883367Z","shell.execute_reply":"2023-06-16T00:36:42.298081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n<br>\n<div style=\"background-color:#20BEFF;\n            border-radius: 7px;\n            padding: 21px;\n            font-family: 'Roboto', sans-serif;\n            font-size:18px;\n            line-height: 1.7em;\">\n    <p style=\"color:white;\n              text-align: center;\">Thank you for coming here! I hope you enjoyed my notebook, and please consider liking or sharing it to make it more accessible to others! Feel free to leave a comment; both positive and constructive feedback will be valuable to me! You are always welcome here!</p></div>\n\n<center><img src='https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExODFkM2Q5N2NhMDVmYTNmYjA2YTI3NTk0YjJiZmNjZjgyNTYwZWQ0ZCZlcD12MV9pbnRlcm5hbF9naWZzX2dpZklkJmN0PXRz/U8NDCmPOnqcEYl1oav/giphy.gif' \n     height=54px width=288px font=50px /></center>    \n","metadata":{"papermill":{"duration":0.029923,"end_time":"2023-06-14T22:58:26.658068","exception":false,"start_time":"2023-06-14T22:58:26.628145","status":"completed"},"tags":[]}}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load necessary library and module\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob, Word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and show sample data\n",
    "mlbb = pd.read_csv('dataset/baruambil-20062022-MLBB.csv', encoding='ISO-8859-1')\n",
    "hdi = pd.read_csv('dataset/baruambil-20062022-HDI.csv', encoding='ISO-8859-1')\n",
    "ffm = pd.read_csv('dataset/baruambil-20062022-FreeFireMax.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Apply text pre-processing to DataFrame\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "snowball = nltk.stem.SnowballStemmer('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "nltk.download('words')\n",
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "  text = text.lower()                                   # Lowercase all sentences\n",
    "      #   text = text.strip()                                   # Remove whitespace\n",
    "  text = re.sub('[-+]?[0-9]+', ' ', text)               # Remove numbers \n",
    "  text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)    # Remove URLs\n",
    "      #   text = re.sub(r\"pic.twitter.com\\S+\", ' ', text)       # Remove custom URLs for twitter\n",
    "      #   text = re.sub(r'\\@([\\w]+)',' ', text)                 # Remove Mention @\n",
    "      #   text = re.sub(r'\\#([\\w]+)',' ', text)                 # Remove #TAGAR\n",
    "  text = re.sub('\\S*@\\S*\\s?', ' ', text)                # Remove email\n",
    "  text = re.sub(r'[^\\w\\s]', ' ', text)                  # Remove punctuation\n",
    "      #   text = re.sub(r'\\b\\w{1,3}\\b','',text)                 #Remove n-chars,Remove less than 3 chars, minimum 4 character allowed \"\\b[a-zA-Z0-9]{3}\\b\"\n",
    "  text = re.sub(r'[!$%^&*@#()_+|~=`{}\\[\\]%\\-:\";\\'<>?,.\\/]', ' ', text)  # Tahap-5: simbol\n",
    "      #   text = re.sub(r'[0-9]+','', text)                     # Tahap-6: angka\n",
    "  text = re.sub(r'([a-zA-Z])\\1\\1','\\\\1', text)          # Tahap-7: koreksi duplikasi tiga karakter beruntun atau lebih (contoh. yukkk)\n",
    "  text = re.sub(' +',' ', text)                         #remove multiple whitespace\n",
    "  text = re.sub(r'^[ ]|[ ]$','', text)                  # Tahap-9: spasi di awal dan akhir kalimat\n",
    "\n",
    "  # text = re.sub('\\b[a-zA-Z0-9]{3}\\b','',text)\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') # Remove non-ascii character\n",
    "  word_tokens = word_tokenize(text) # Word tokenize\n",
    "  \n",
    "\n",
    "  # Define Indonesian stopwords removal\n",
    "  stop_words = stopwords.words('english')  # NLTK Indonesian stopwords\n",
    "  clean_words = [word for word in word_tokens if word not in stop_words] # stopwords removal\n",
    "  clean_words = ' '.join(clean_words)\n",
    "\n",
    "  words = set(nltk.corpus.words.words())\n",
    "  word_cleaner = word_tokenize(clean_words)\n",
    "  cleaner_words = [w for w in word_cleaner if w.lower() in words or not w.isalpha()] #remove non english\n",
    "  cleaner = ' '.join(cleaner_words)\n",
    "\n",
    "  #stemming with snowball update 07072022 stemmer terakhir\n",
    "#   stem_token=word_tokenize(cleaner)\n",
    "#   stem_words = [snowball.stem(w) for w in stem_token]\n",
    "#   stem_clean= ' '.join(stem_words)\n",
    "\n",
    "  #update13072022 pake lemmatization TextBlob\n",
    "  tblob=TextBlob(clean_words)\n",
    "  lemma= [Word(word).lemmatize(\"v\") for word in tblob.words]\n",
    "  lemma_clean= ' '.join(lemma)\n",
    "\n",
    "  #   cleaner = \" \".join(w for w in nltk.wordpunct_tokenize(clean_words) \\\n",
    "        #   if w.lower() in words or not w.isalpha())\n",
    "\n",
    "  return lemma_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea341305cbcc491896225ea29a1c9891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\!MYDOCUMENTS2021\\!!BILLY-2022\\!!PYTHON\\!!CONTOHPERINTIS2022\\1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/%21MYDOCUMENTS2021/%21%21BILLY-2022/%21%21PYTHON/%21%21CONTOHPERINTIS2022/1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb#ch0000005?line=0'>1</a>\u001b[0m mlbb[\u001b[39m'\u001b[39m\u001b[39mclean_content\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m mlbb[\u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mprogress_apply(\u001b[39mlambda\u001b[39;49;00m x: text_preprocessing(x))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%21MYDOCUMENTS2021/%21%21BILLY-2022/%21%21PYTHON/%21%21CONTOHPERINTIS2022/1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb#ch0000005?line=1'>2</a>\u001b[0m ffm[\u001b[39m'\u001b[39m\u001b[39mclean_content\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m ffm[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mprogress_apply(\u001b[39mlambda\u001b[39;00m x: text_preprocessing(x))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%21MYDOCUMENTS2021/%21%21BILLY-2022/%21%21PYTHON/%21%21CONTOHPERINTIS2022/1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb#ch0000005?line=2'>3</a>\u001b[0m hdi[\u001b[39m'\u001b[39m\u001b[39mclean_content\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m hdi[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mprogress_apply(\u001b[39mlambda\u001b[39;00m x: text_preprocessing(x))\n",
      "File \u001b[1;32md:\\AnacondaPython3\\lib\\site-packages\\tqdm\\std.py:814\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[39m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[39m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 814\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(df, df_function)(wrapper, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    815\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    816\u001b[0m     t\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\AnacondaPython3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32md:\\AnacondaPython3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32md:\\AnacondaPython3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1138\u001b[0m             values,\n\u001b[0;32m   1139\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1141\u001b[0m         )\n\u001b[0;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32md:\\AnacondaPython3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\AnacondaPython3\\lib\\site-packages\\tqdm\\std.py:809\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    804\u001b[0m     \u001b[39m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    805\u001b[0m     \u001b[39m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    806\u001b[0m     \u001b[39m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[39m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     t\u001b[39m.\u001b[39mupdate(n\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39mtotal \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mn \u001b[39m<\u001b[39m t\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[1;32m--> 809\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32md:\\!MYDOCUMENTS2021\\!!BILLY-2022\\!!PYTHON\\!!CONTOHPERINTIS2022\\1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb Cell 6'\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/%21MYDOCUMENTS2021/%21%21BILLY-2022/%21%21PYTHON/%21%21CONTOHPERINTIS2022/1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb#ch0000005?line=0'>1</a>\u001b[0m mlbb[\u001b[39m'\u001b[39m\u001b[39mclean_content\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m mlbb[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mprogress_apply(\u001b[39mlambda\u001b[39;00m x: text_preprocessing(x))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%21MYDOCUMENTS2021/%21%21BILLY-2022/%21%21PYTHON/%21%21CONTOHPERINTIS2022/1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb#ch0000005?line=1'>2</a>\u001b[0m ffm[\u001b[39m'\u001b[39m\u001b[39mclean_content\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m ffm[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mprogress_apply(\u001b[39mlambda\u001b[39;00m x: text_preprocessing(x))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%21MYDOCUMENTS2021/%21%21BILLY-2022/%21%21PYTHON/%21%21CONTOHPERINTIS2022/1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb#ch0000005?line=2'>3</a>\u001b[0m hdi[\u001b[39m'\u001b[39m\u001b[39mclean_content\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m hdi[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mprogress_apply(\u001b[39mlambda\u001b[39;00m x: text_preprocessing(x))\n",
      "\u001b[1;32md:\\!MYDOCUMENTS2021\\!!BILLY-2022\\!!PYTHON\\!!CONTOHPERINTIS2022\\1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb Cell 5'\u001b[0m in \u001b[0;36mtext_preprocessing\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%21MYDOCUMENTS2021/%21%21BILLY-2022/%21%21PYTHON/%21%21CONTOHPERINTIS2022/1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb#ch0000004?line=24'>25</a>\u001b[0m clean_words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m word_tokens \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words] \u001b[39m# stopwords removal\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%21MYDOCUMENTS2021/%21%21BILLY-2022/%21%21PYTHON/%21%21CONTOHPERINTIS2022/1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb#ch0000004?line=25'>26</a>\u001b[0m clean_words \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(clean_words)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%21MYDOCUMENTS2021/%21%21BILLY-2022/%21%21PYTHON/%21%21CONTOHPERINTIS2022/1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb#ch0000004?line=27'>28</a>\u001b[0m words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(nltk\u001b[39m.\u001b[39mcorpus\u001b[39m.\u001b[39mwords\u001b[39m.\u001b[39mwords())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%21MYDOCUMENTS2021/%21%21BILLY-2022/%21%21PYTHON/%21%21CONTOHPERINTIS2022/1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb#ch0000004?line=28'>29</a>\u001b[0m word_cleaner \u001b[39m=\u001b[39m word_tokenize(clean_words)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%21MYDOCUMENTS2021/%21%21BILLY-2022/%21%21PYTHON/%21%21CONTOHPERINTIS2022/1_Preprocessing-Lemmatization-BAB3-Billy-13072022.ipynb#ch0000004?line=29'>30</a>\u001b[0m cleaner_words \u001b[39m=\u001b[39m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m word_cleaner \u001b[39mif\u001b[39;00m w\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m words \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m w\u001b[39m.\u001b[39misalpha()] \u001b[39m#remove non english\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mlbb['clean_content'] = mlbb['content'].progress_apply(lambda x: text_preprocessing(x))\n",
    "ffm['clean_content'] = ffm['content'].progress_apply(lambda x: text_preprocessing(x))\n",
    "hdi['clean_content'] = hdi['content'].progress_apply(lambda x: text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlbb.to_csv( \"hasil/1_mlbb_clean_content_lemma_13072022_bab4.csv\", index=False, encoding='utf-8-sig')\n",
    "ffm.to_csv( \"hasil/1_ffm_clean_content_lemma_13072022_bab4.csv\", index=False, encoding='utf-8-sig')\n",
    "hdi.to_csv( \"hasil/1_hdi_clean_content_lemma_13072022_bab4.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "678bcfd6376b9a1d259d9993fe75604a4b4f3b2c5ff5a71eca36094cc19bcb7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

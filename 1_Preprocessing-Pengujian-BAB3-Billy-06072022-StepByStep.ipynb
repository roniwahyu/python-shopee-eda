{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Load necessary library and module\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and show sample data\n",
    "mlbb = pd.read_csv('dataset/baruambil-20062022-MLBB.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply text pre-processing to DataFrame\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "snowball = nltk.stem.SnowballStemmer('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casefolding(text):\n",
    "    text_clean = text.lower()\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67cda3fad0c44cb480547f6199ae0cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlbb['case_folding'] = mlbb['content'].progress_apply(lambda x: casefolding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlbb.to_csv( \"hasil/1_testmanualisasi_casefolding.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtertext(text):\n",
    "    text = text.strip()                                   # Remove whitespace\n",
    "    text = re.sub('[-+]?[0-9]+', ' ', text)               # Remove numbers \n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)    # Remove URLs\n",
    "    text = re.sub(r\"pic.twitter.com\\S+\", ' ', text)       # Remove custom URLs for twitter\n",
    "    text = re.sub(r'\\@([\\w]+)',' ', text)                 # Remove Mention @\n",
    "    text = re.sub(r'\\#([\\w]+)',' ', text)                 # Remove #TAGAR\n",
    "    text = re.sub('\\S*@\\S*\\s?', ' ', text)                # Remove email\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9d510ea0444127a6c2f15334df9b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlbb['filter_text'] = mlbb['case_folding'].progress_apply(lambda x: filtertext(x))\n",
    "mlbb.to_csv( \"hasil/1_testmanualisasi_filter1.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)                  # Remove punctuation\n",
    "    text = re.sub(r'[!$%^&*@#()_+|~=`{}\\[\\]%\\-:\";\\'<>?,.\\/]', ' ', text)  # Tahap-5: simbol\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed6258dcaaf4fe683f0cb254ffaf634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlbb['filter_punctuation'] = mlbb['filter_text'].progress_apply(lambda x: filter_punctuation(x))\n",
    "mlbb.to_csv( \"hasil/1_testmanualisasi_filter1.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_space(text):\n",
    "    text = re.sub(r'([a-zA-Z])\\1\\1','\\\\1', text)          # Tahap-7: koreksi duplikasi tiga karakter beruntun atau lebih (contoh. yukkk)\n",
    "    text = re.sub(' +',' ', text)                         # remove multiple whitespace\n",
    "    text = re.sub(r'^[ ]|[ ]$','', text)                  # Tahap-9: spasi di awal dan akhir kalimat\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b442812dc48e4d5a9534dcbaee98a739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlbb['filter_space'] = mlbb['filter_punctuation'].progress_apply(lambda x: filter_space(x))\n",
    "mlbb.to_csv( \"hasil/1_testmanualisasi_filter1.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ascii(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') # Remove non-ascii character\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c52132f79e0414981e9bd458d73fa6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlbb['filter_ascii'] = mlbb['filter_space'].progress_apply(lambda x: filter_ascii(x))\n",
    "mlbb.to_csv( \"hasil/1_testmanualisasi_filter1.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisasi(text):\n",
    "    word_tokens = word_tokenize(text) # Word tokenize\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616a55ca85414c19a27a0fd00bf2bad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlbb['tokenisasi_content'] = mlbb['content'].progress_apply(lambda x: tokenisasi(x))\n",
    "mlbb.to_csv( \"hasil/1_testmanualisasi_filter1.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonenglish(text):\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    text_token = word_tokenize(text)\n",
    "    cleaner_words = [w for w in text_token if w.lower() in words or not w.isalpha()] #remove non english\n",
    "    cleaner = ' '.join(cleaner_words)\n",
    "    return cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordfilter(text):\n",
    "    # Define Indonesian stopwords removal\n",
    "    word_tokens = word_tokenize(text) # Word tokenize\n",
    "    stop_words = stopwords.words('english')  # NLTK Indonesian stopwords\n",
    "    clean_words = [word for word in word_tokens if word not in stop_words] # stopwords removal\n",
    "    clean_words = ' '.join(clean_words)\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f6d284fd304dd9bcccdcac727f8035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlbb['stopwordfilter'] = mlbb['filter_ascii'].progress_apply(lambda x:stopwordfilter(x))\n",
    "mlbb.to_csv( \"hasil/1_testmanualisasi_filter1.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    word_tokens = word_tokenize(text) # Word tokenize\n",
    "    stem_words = [snowball.stem(w) for w in word_tokens]\n",
    "    cleanword= ' '.join(stem_words)\n",
    "    return cleanword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377a6e33a2df47afb557505fb10b14c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlbb['stemming_word'] = mlbb['stopwordfilter'].progress_apply(lambda x:stemming(x))\n",
    "mlbb.to_csv( \"hasil/1_testmanualisasi_filter1.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895fa521149d48129c4729b91cf293f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlbb['filter_noneng_baru'] = mlbb['stemming_word'].progress_apply(lambda x:nonenglish(x))\n",
    "mlbb.to_csv( \"hasil/1_testmanualisasi_filter1.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "  text = text.lower()                                   # Lowercase all sentences\n",
    "  text = text.strip()                                   # Remove whitespace\n",
    "  text = re.sub('[-+]?[0-9]+', ' ', text)               # Remove numbers \n",
    "  text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)    # Remove URLs\n",
    "  text = re.sub(r\"pic.twitter.com\\S+\", ' ', text)       # Remove custom URLs for twitter\n",
    "  text = re.sub(r'\\@([\\w]+)',' ', text)                 # Remove Mention @\n",
    "  text = re.sub(r'\\#([\\w]+)',' ', text)                 # Remove #TAGAR\n",
    "  text = re.sub('\\S*@\\S*\\s?', ' ', text)                # Remove email\n",
    "  text = re.sub(r'[^\\w\\s]', ' ', text)                  # Remove punctuation\n",
    "  #text = re.sub(r'\\b\\w{1,3}\\b','',text)                 #Remove n-chars,Remove less than 3 chars, minimum 4 character allowed \"\\b[a-zA-Z0-9]{3}\\b\"\n",
    "  text = re.sub(r'[!$%^&*@#()_+|~=`{}\\[\\]%\\-:\";\\'<>?,.\\/]', ' ', text)  # Tahap-5: simbol\n",
    "  text = re.sub(r'([a-zA-Z])\\1\\1','\\\\1', text)          # Tahap-7: koreksi duplikasi tiga karakter beruntun atau lebih (contoh. yukkk)\n",
    "  text = re.sub(' +',' ', text)                         #remove multiple whitespace\n",
    "  text = re.sub(r'^[ ]|[ ]$','', text)                  # Tahap-9: spasi di awal dan akhir kalimat\n",
    "\n",
    "  # text = re.sub('\\b[a-zA-Z0-9]{3}\\b','',text)\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') # Remove non-ascii character\n",
    "  word_tokens = word_tokenize(text) # Word tokenize\n",
    "  \n",
    "  \n",
    "  \n",
    "  # Define Indonesian stopwords removal\n",
    "  stop_words = stopwords.words('english')  # NLTK Indonesian stopwords\n",
    "  clean_words = [word for word in word_tokens if word not in stop_words] # stopwords removal\n",
    "  clean_words = ' '.join(clean_words)\n",
    "\n",
    "  words = set(nltk.corpus.words.words())\n",
    "  word_cleaner = word_tokenize(clean_words)\n",
    "  cleaner_words = [w for w in word_cleaner if w.lower() in words or not w.isalpha()] #remove non english\n",
    "  cleaner = ' '.join(cleaner_words)\n",
    "\n",
    "  #stemming with porter\n",
    "  stem_token=word_tokenize(cleaner)\n",
    "  stem_words = [snowball.stem(w) for w in stem_token]\n",
    "  stem_clean= ' '.join(stem_words)\n",
    "\n",
    "  #   cleaner = \" \".join(w for w in nltk.wordpunct_tokenize(clean_words) \\\n",
    "        #   if w.lower() in words or not w.isalpha())\n",
    "\n",
    "  return stem_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5358a54a8f7044b3bafed37fdbc4c029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlbb['clean_content'] = mlbb['content'].progress_apply(lambda x: text_preprocessing(x))\n",
    "mlbb.to_csv( \"hasil/mlbb_clean_content_after_preprocessing_07072022_bab4.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "678bcfd6376b9a1d259d9993fe75604a4b4f3b2c5ff5a71eca36094cc19bcb7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
